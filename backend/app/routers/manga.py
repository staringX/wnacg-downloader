from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List
from datetime import datetime
from pydantic import BaseModel
import os
import time
import uuid
from app.database import get_db

# å¯é€‰çš„Seleniumå¯¼å…¥
try:
    from selenium.webdriver.common.by import By
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("è­¦å‘Š: Seleniumæœªå®‰è£…ï¼Œçˆ¬è™«åŠŸèƒ½å°†ä¸å¯ç”¨")
from app.models import Manga
from app.schemas import (
    MangaResponse, SyncResponse, DownloadResponse, 
    BatchDownloadResponse, MangaUpdate
)
from app.crawler.base import MangaCrawler
from app.utils.downloader import MangaDownloader
from app.config import settings

router = APIRouter(prefix="/api", tags=["manga"])


@router.get("/mangas", response_model=List[MangaResponse])
def get_mangas(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰æ¼«ç”»"""
    mangas = db.query(Manga).all()
    return [MangaResponse.from_orm(manga) for manga in mangas]


@router.post("/sync", response_model=SyncResponse)
def sync_collection(background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """åŒæ­¥æ”¶è—å¤¹"""
    if not SELENIUM_AVAILABLE:
        raise HTTPException(status_code=503, detail="Seleniumæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨çˆ¬è™«åŠŸèƒ½")
    
    crawler = MangaCrawler()
    
    try:
        # ç™»å½•
        if not crawler.login(settings.manga_username, settings.manga_password):
            raise HTTPException(status_code=401, detail="ç™»å½•å¤±è´¥")
        
        # ğŸš€ ä½¿ç”¨ç”Ÿæˆå™¨ï¼šè¾¹çˆ¬å–è¾¹ä¿å­˜ï¼ŒçœŸæ­£çš„å®æ—¶åŒæ­¥ï¼
        print(f"å¼€å§‹å®æ—¶åŒæ­¥ï¼ˆç”Ÿæˆå™¨æ¨¡å¼ï¼‰...")
        print(f"æç¤ºï¼šæ¯çˆ¬å–åˆ°ä¸€ä¸ªæ¼«ç”»å°±ä¼šç«‹å³ä¿å­˜ï¼Œåˆ·æ–°é¡µé¢å³å¯çœ‹åˆ°æœ€æ–°æ•°æ®\n")
        
        added_count = 0
        updated_count = 0
        processed_count = 0
        
        # ç”Ÿæˆå™¨ï¼šæ¯yieldä¸€ä¸ªæ¼«ç”»ï¼Œç«‹å³å¤„ç†å¹¶ä¿å­˜
        for item in crawler.get_collection_stream():
            processed_count += 1
            
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = db.query(Manga).filter(Manga.manga_url == item['manga_url']).first()
                
                if existing:
                    # å·²å­˜åœ¨ï¼Œä»…æ›´æ–°åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if item.get('page_count') and not existing.page_count:
                        existing.page_count = item['page_count']
                        db.commit()
                    updated_count += 1
                    print(f"  [{processed_count}] âŸ³ å·²å­˜åœ¨: {item['title'][:50]}")
                else:
                    # æ–°æ¼«ç”»ï¼Œåˆ›å»ºè®°å½•å¹¶ç«‹å³ä¿å­˜
                    print(f"  [{processed_count}] âœš æ–°å¢: {item['title'][:50]}")
                    
                    manga = Manga(
                        title=item['title'],
                        author=item['author'],
                        manga_url=item['manga_url'],
                        page_count=item.get('page_count')
                    )
                    db.add(manga)
                    db.commit()  # ğŸ”¥ ç«‹å³æäº¤ï¼ç”¨æˆ·åˆ·æ–°é¡µé¢å°±èƒ½çœ‹åˆ°
                    db.refresh(manga)  # åˆ·æ–°å¯¹è±¡ä»¥è·å–ID
                    
                    added_count += 1
                    
                    # ç«‹å³è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆé¡µæ•°ã€æ›´æ–°æ—¥æœŸã€å°é¢ï¼‰
                    try:
                        details = crawler.get_manga_details(manga.manga_url)
                        
                        if details:
                            # æ›´æ–°è¯¦ç»†ä¿¡æ¯
                            if details.get('page_count'):
                                manga.page_count = details['page_count']
                            if details.get('updated_at'):
                                manga.updated_at = details['updated_at']
                            if details.get('cover_image_url'):
                                manga.cover_image_url = details['cover_image_url']
                            db.commit()  # ğŸ”¥ å†æ¬¡æäº¤è¯¦æƒ…ï¼
                            print(f"       âœ“ è¯¦æƒ…: é¡µæ•°={manga.page_count}, æ›´æ–°={str(manga.updated_at)[:10] if manga.updated_at else 'N/A'}")
                        else:
                            print(f"       âš  æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
                            
                    except Exception as detail_error:
                        print(f"       âš  è·å–è¯¦æƒ…å¤±è´¥: {detail_error}")
                        # è¯¦æƒ…è·å–å¤±è´¥ä¸å½±å“åŸºæœ¬è®°å½•çš„ä¿å­˜ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    
            except Exception as e:
                print(f"  [{processed_count}] âœ— å¤„ç†å¤±è´¥: {item.get('title', 'Unknown')[:50]} - {e}")
                db.rollback()  # å›æ»šå½“å‰å¤±è´¥çš„äº‹åŠ¡
                continue
        
        print(f"\nåŒæ­¥å®Œæˆï¼šæ–°å¢ {added_count} ä¸ªï¼Œæ›´æ–° {updated_count} ä¸ª")
        
        return SyncResponse(
            success=True,
            message=f"åŒæ­¥å®Œæˆï¼šæ–°å¢ {added_count} ä¸ªï¼Œæ›´æ–° {updated_count} ä¸ª",
            added_count=added_count,
            updated_count=updated_count
        )
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        crawler.close()


@router.post("/download/{manga_id}", response_model=DownloadResponse)
def download_manga(manga_id: str, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """ä¸‹è½½å•ä¸ªæ¼«ç”»"""
    manga = db.query(Manga).filter(Manga.id == manga_id).first()
    if not manga:
        raise HTTPException(status_code=404, detail="æ¼«ç”»ä¸å­˜åœ¨")
    
    if manga.is_downloaded:
        return DownloadResponse(
            success=True,
            message="æ¼«ç”»å·²ä¸‹è½½",
            manga_id=manga_id,
            file_path=manga.cbz_file_path
        )
    
    crawler = MangaCrawler()
    downloader = MangaDownloader()
    
    try:
        # ç™»å½•
        if not crawler.login(settings.manga_username, settings.manga_password):
            raise HTTPException(status_code=401, detail="ç™»å½•å¤±è´¥")
        
        # è·å–æ¼«ç”»è¯¦æƒ…ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
        if not manga.page_count or not manga.cover_image_url:
            details = crawler.get_manga_details(manga.manga_url)
            if details:
                if details.get('page_count'):
                    manga.page_count = details['page_count']
                if details.get('updated_at'):
                    manga.updated_at = details['updated_at']
                if details.get('cover_image_url'):
                    manga.cover_image_url = details['cover_image_url']
        
        # è·å–å›¾ç‰‡åˆ—è¡¨
        images = crawler.get_manga_images(manga.manga_url)
        
        if not images:
            raise HTTPException(status_code=500, detail="æ— æ³•è·å–å›¾ç‰‡åˆ—è¡¨")
        
        # ä¸‹è½½å¹¶æ‰“åŒ…
        cbz_path, cover_path = downloader.download_manga(manga.title, images)
        
        if not cbz_path:
            raise HTTPException(status_code=500, detail="ä¸‹è½½å¤±è´¥")
        
        # æ›´æ–°æ•°æ®åº“
        manga.is_downloaded = True
        manga.downloaded_at = datetime.now()
        manga.cbz_file_path = cbz_path
        manga.cover_image_path = cover_path
        manga.file_size = downloader.get_file_size(cbz_path)
        
        db.commit()
        
        return DownloadResponse(
            success=True,
            message="ä¸‹è½½æˆåŠŸ",
            manga_id=manga_id,
            file_path=cbz_path
        )
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        crawler.close()


class BatchDownloadRequest(BaseModel):
    manga_ids: List[str]


@router.post("/download/batch", response_model=BatchDownloadResponse)
def download_batch(request: BatchDownloadRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """æ‰¹é‡ä¸‹è½½æ¼«ç”»"""
    success_count = 0
    failed_count = 0
    
    for manga_id in request.manga_ids:
        try:
            download_manga(manga_id, background_tasks, db)
            success_count += 1
        except:
            failed_count += 1
    
    return BatchDownloadResponse(
        success=True,
        message=f"æ‰¹é‡ä¸‹è½½å®Œæˆï¼šæˆåŠŸ {success_count}ï¼Œå¤±è´¥ {failed_count}",
        total=len(request.manga_ids),
        success_count=success_count,
        failed_count=failed_count
    )


@router.get("/recent-updates", response_model=List[MangaResponse])
def get_recent_updates(db: Session = Depends(get_db)):
    """è·å–æœ€è¿‘æ›´æ–°ï¼ˆæ”¶è—ä½œè€…çš„æœ€è¿‘æ›´æ–°ï¼‰"""
    # è·å–æ‰€æœ‰å·²æ”¶è—çš„ä½œè€…
    authors = db.query(Manga.author).distinct().all()
    author_list = [a[0] for a in authors]
    
    if not author_list:
        return []
    
    # è·å–æ¯ä¸ªä½œè€…æ”¶è—å¤¹ä¸­æœ€æ–°çš„æ¼«ç”»çš„æ›´æ–°æ—¥æœŸ
    author_latest_dates = {}
    for author in author_list:
        latest_manga = db.query(Manga).filter(
            Manga.author == author
        ).order_by(Manga.updated_at.desc()).first()
        
        if latest_manga and latest_manga.updated_at:
            author_latest_dates[author] = latest_manga.updated_at
    
    # æœç´¢æ¯ä¸ªä½œè€…ï¼Œè·å–æ›´æ–°æ—¥æœŸæ™šäºæ”¶è—å¤¹æœ€æ–°æ¼«ç”»çš„æ‰€æœ‰æ¼«ç”»
    crawler = MangaCrawler()
    recent_updates = []
    
    try:
        if not crawler.login(settings.manga_username, settings.manga_password):
            return []
        
        for author in author_list:
            # æœç´¢ä½œè€…
            search_url = f"{crawler.base_url}/search-index.html?keywords={author}"
            crawler.driver.get(search_url)
            time.sleep(2)
            
            # è·å–æœç´¢ç»“æœ
            manga_items = crawler.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index']")
            
            for item in manga_items:
                try:
                    manga_url = item.get_attribute('href')
                    if not manga_url:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦å·²åœ¨æ”¶è—å¤¹ä¸­
                    existing = db.query(Manga).filter(
                        Manga.manga_url == manga_url
                    ).first()
                    
                    if existing:
                        continue  # å·²åœ¨æ”¶è—å¤¹ä¸­ï¼Œè·³è¿‡
                    
                    details = crawler.get_manga_details(manga_url)
                    
                    if details:
                        latest_date = author_latest_dates.get(author)
                        if latest_date and details.get('updated_at'):
                            if details['updated_at'] > latest_date:
                                # åˆ›å»ºä¸´æ—¶Mangaå¯¹è±¡ç”¨äºè¿”å›
                                temp_manga = Manga(
                                    id=str(uuid.uuid4()),
                                    title=details['title'],
                                    author=author,
                                    manga_url=manga_url,
                                    page_count=details.get('page_count'),
                                    updated_at=details.get('updated_at'),
                                    cover_image_url=details.get('cover_image_url'),
                                    is_downloaded=False
                                )
                                recent_updates.append(temp_manga)
                except Exception as e:
                    print(f"å¤„ç†æœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                    continue
    except Exception as e:
        print(f"è·å–æœ€è¿‘æ›´æ–°å¤±è´¥: {e}")
    finally:
        crawler.close()
    
    # è½¬æ¢ä¸ºå“åº”æ ¼å¼
    return [MangaResponse.from_orm(manga) for manga in recent_updates]


class AddToCollectionRequest(BaseModel):
    manga_url: str
    author: str


@router.post("/add-to-collection")
def add_to_collection(request: AddToCollectionRequest, db: Session = Depends(get_db)):
    """æ·»åŠ æ¼«ç”»åˆ°æ”¶è—å¤¹ï¼ˆå¯¹åº”ä½œè€…åˆ†ç±»ï¼‰"""
    crawler = MangaCrawler()
    
    try:
        if not crawler.login(settings.manga_username, settings.manga_password):
            raise HTTPException(status_code=401, detail="ç™»å½•å¤±è´¥")
        
        # å¯¼èˆªåˆ°æ¼«ç”»é¡µé¢
        crawler.driver.get(request.manga_url)
        time.sleep(2)
        
        # æŸ¥æ‰¾"åŠ å…¥æ›¸æ¶"æŒ‰é’®
        add_button = crawler.driver.find_element(By.XPATH, "//*[contains(text(), 'åŠ å…¥æ›¸æ¶')]")
        add_button.click()
        time.sleep(1)
        
        # é€‰æ‹©ä½œè€…åˆ†ç±»
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç½‘ç«™UIæ¥å®ç°
        # å¯èƒ½éœ€è¦æ‰“å¼€ä¸‹æ‹‰èœå•é€‰æ‹©åˆ†ç±»
        
        # æ·»åŠ åˆ°æ•°æ®åº“
        details = crawler.get_manga_details(request.manga_url)
        if details:
            manga = Manga(
                title=details['title'],
                author=request.author,
                manga_url=request.manga_url,
                page_count=details.get('page_count'),
                updated_at=details.get('updated_at'),
                cover_image_url=details.get('cover_image_url')
            )
            db.add(manga)
            db.commit()
            
            return {"success": True, "message": "å·²æ·»åŠ åˆ°æ”¶è—å¤¹"}
        else:
            raise HTTPException(status_code=500, detail="æ— æ³•è·å–æ¼«ç”»è¯¦æƒ…")
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        crawler.close()


@router.delete("/manga/{manga_id}")
def delete_manga(manga_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤æ¼«ç”»"""
    manga = db.query(Manga).filter(Manga.id == manga_id).first()
    if not manga:
        raise HTTPException(status_code=404, detail="æ¼«ç”»ä¸å­˜åœ¨")
    
    # åˆ é™¤æ–‡ä»¶
    if manga.cbz_file_path and os.path.exists(manga.cbz_file_path):
        os.remove(manga.cbz_file_path)
    if manga.cover_image_path and os.path.exists(manga.cover_image_path):
        os.remove(manga.cover_image_path)
    
    db.delete(manga)
    db.commit()
    
    return {"success": True, "message": "åˆ é™¤æˆåŠŸ"}
