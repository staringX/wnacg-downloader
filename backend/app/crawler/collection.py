"""æ”¶è—å¤¹çˆ¬å–æ¨¡å—"""
import time
import re
from typing import Dict, Generator
from selenium.webdriver.common.by import By
from app.utils.logger import logger


class CollectionCrawler:
    """æ”¶è—å¤¹çˆ¬å–å™¨ - è´Ÿè´£ä»æ”¶è—å¤¹è·å–æ¼«ç”»åˆ—è¡¨"""
    
    def __init__(self, browser_manager):
        self.browser = browser_manager
        self.driver = browser_manager.driver
        self.base_url = browser_manager.base_url
    
    def get_collection_stream(self) -> Generator[Dict, None, None]:
        """
        è·å–æ”¶è—å¤¹ä¸­çš„æ‰€æœ‰æ¼«ç”»ï¼ˆç”Ÿæˆå™¨ç‰ˆæœ¬ï¼‰
        è¾¹çˆ¬å–è¾¹è¿”å›ï¼Œä¸ç­‰å¾…å…¨éƒ¨å®Œæˆï¼Œå®ç°çœŸæ­£çš„å®æ—¶åŒæ­¥
        
        Yields:
            dict: æ¼«ç”»ä¿¡æ¯å­—å…¸ {'title', 'author', 'manga_url', 'page_count'}
        """
        if not self.driver:
            return
        
        try:
            manga_urls_set = set()  # ç”¨äºå»é‡
            base = self.base_url.rstrip('/')
            
            # æ­£ç¡®çš„ä¹¦æ¶URL
            bookshelf_url = f"{base}/users-users_fav.html"
            logger.info(f"è®¿é—®ä¹¦æ¶é¡µé¢: {bookshelf_url}")
            self.driver.get(bookshelf_url)
            time.sleep(5)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æˆåŠŸåŠ è½½
            current_url = self.driver.current_url
            page_title = self.driver.title
            logger.info(f"å½“å‰é¡µé¢URL: {current_url}")
            logger.info(f"é¡µé¢æ ‡é¢˜: {page_title}")
            
            if "404" in page_title.lower() or "404" in self.driver.page_source[:1000].lower():
                logger.warning(f"ä¹¦æ¶é¡µé¢è¿”å›404")
                return
            
            # æŸ¥æ‰¾åˆ†ç±»é“¾æ¥
            category_links = {}
            all_links = self.driver.find_elements(By.CSS_SELECTOR, "a")
            
            for link in all_links:
                href = link.get_attribute('href') or ''
                text = link.text.strip()
                
                if 'users-users_fav-c-' in href and text:
                    if text not in ["å…¨éƒ¨", "ç®¡ç†åˆ†é¡", "æ›¸æ¶", "ä¹¦æ¶", "æˆ‘çš„æ›¸æ¶"]:
                        category_links[text] = href
                        logger.info(f"æ‰¾åˆ°åˆ†ç±»: {text} -> {href}")
            
            logger.info(f"å…±æ‰¾åˆ° {len(category_links)} ä¸ªä½œè€…åˆ†ç±»\n")
            
            total_count = 0
            
            # å¦‚æœæœ‰åˆ†ç±»ï¼ŒæŒ‰åˆ†ç±»è·å–æ¼«ç”»
            if category_links:
                for author_idx, (author, category_url) in enumerate(category_links.items(), 1):
                    logger.info(f"[{author_idx}/{len(category_links)}] å¤„ç†ä½œè€…åˆ†ç±»: {author}")
                    
                    # æå–åˆ†ç±»ID
                    category_id_match = re.search(r'users-users_fav-c-(\d+)\.html', category_url)
                    if not category_id_match:
                        logger.warning(f"  æ— æ³•æå–åˆ†ç±»IDï¼Œè·³è¿‡")
                        continue
                    
                    category_id = category_id_match.group(1)
                    page_num = 1
                    author_manga_count = 0
                    
                    current_url = category_url
                    visited_urls = set()
                    
                    # éå†æ‰€æœ‰åˆ†é¡µ
                    while True:
                        if current_url in visited_urls:
                            logger.info(f"  æ£€æµ‹åˆ°é‡å¤URLï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        logger.info(f"  è®¿é—®ç¬¬ {page_num} é¡µ: {current_url}")
                        self.driver.get(current_url)
                        visited_urls.add(current_url)
                        time.sleep(2)
                        
                        # æŸ¥æ‰¾è¯¥é¡µé¢ä¸‹çš„æ‰€æœ‰æ¼«ç”»é“¾æ¥
                        manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                        logger.info(f"    ğŸ” CSSæ‰¾åˆ° {len(manga_links)} ä¸ªé“¾æ¥")
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç«‹å³æå–æ‰€æœ‰é“¾æ¥ä¿¡æ¯ï¼Œé¿å…stale element reference
                        manga_info_list = []
                        for manga_link in manga_links:
                            try:
                                manga_url = manga_link.get_attribute('href')
                                title = manga_link.text.strip()
                                
                                # å°è¯•è·å–é¡µæ•°ä¿¡æ¯ï¼ˆä½¿ç”¨classåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                                page_count = None
                                try:
                                    # æŸ¥æ‰¾åŒ…å«æ¼«ç”»é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œç„¶åæŸ¥æ‰¾ p.l_detla å…ƒç´ 
                                    parent_container = manga_link.find_element(By.XPATH, "./ancestor::*[contains(@class, 'u_listcon') or contains(@class, 'box_cel')]")
                                    page_elem = parent_container.find_element(By.CSS_SELECTOR, "p.l_detla")
                                    if page_elem:
                                        page_text = page_elem.text
                                        # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                                        page_match = re.search(r'(\d+)\s*P?', page_text)
                                        if page_match:
                                            page_count = int(page_match.group(1))
                                except:
                                    pass
                                
                                if manga_url and title:
                                    manga_info_list.append({
                                        'url': manga_url,
                                        'title': title,
                                        'page_count': page_count
                                    })
                            except Exception as e:
                                # å¦‚æœè·å–ä¿¡æ¯å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªé“¾æ¥
                                continue
                        
                        # ç°åœ¨å¤„ç†æå–çš„ä¿¡æ¯åˆ—è¡¨
                        page_manga_count = 0
                        empty_count = 0
                        dup_count = 0
                        
                        for idx, manga_info in enumerate(manga_info_list, 1):
                            try:
                                manga_url = manga_info['url']
                                title = manga_info['title']
                                page_count = manga_info.get('page_count')
                                
                                if idx <= 3:  # æ‰“å°å‰3ä¸ª
                                    logger.debug(f"      [{idx}] URL={manga_url[-30:]}, Title='{title[:50]}'")
                                
                                if not title or not manga_url:
                                    empty_count += 1
                                    if idx <= 3:
                                        logger.debug(f"      [{idx}] âŒ è·³è¿‡ï¼šæ ‡é¢˜æˆ–URLä¸ºç©º")
                                    continue
                                
                                # å»é‡
                                if manga_url in manga_urls_set:
                                    dup_count += 1
                                    if idx <= 3:
                                        logger.debug(f"      [{idx}] â­ï¸  è·³è¿‡ï¼šé‡å¤")
                                    continue
                                
                                # âœ¨ å…³é”®ï¼šç«‹å³ yieldï¼Œä¸ç­‰å¾…åç»­çˆ¬å–
                                manga_urls_set.add(manga_url)
                                page_manga_count += 1
                                author_manga_count += 1
                                total_count += 1
                                
                                yield {
                                    'title': title,
                                    'author': author,
                                    'manga_url': manga_url,
                                    'page_count': page_count
                                }
                                
                            except Exception as e:
                                logger.warning(f"    å¤„ç†æ¼«ç”»å¤±è´¥: {e}")
                                continue
                        
                        logger.info(f"    ç¬¬ {page_num} é¡µï¼šæ‰¾åˆ° {page_manga_count} ä¸ªæ¼«ç”»ï¼ˆæ€»è®¡: {total_count}ï¼‰")
                        logger.debug(f"    ğŸ“Š è·³è¿‡ï¼šç©ºæ ‡é¢˜/URL={empty_count}, é‡å¤={dup_count}")
                        
                        if page_manga_count == 0:
                            logger.info(f"    ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°æ¼«ç”»ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆä½¿ç”¨HTMLå…ƒç´ å’ŒClassåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                        next_page_url = None
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆæ”¶é›†æ‰€æœ‰å¯èƒ½çš„ç¿»é¡µé“¾æ¥URLï¼Œé¿å…stale element reference
                        candidate_urls = []
                        
                        try:
                            # æ–¹æ³•1ï¼šé€šè¿‡åˆ†é¡µå™¨ç»“æ„æŸ¥æ‰¾ï¼ˆä½¿ç”¨classåç§°ï¼‰
                            paginator = self.driver.find_element(By.CSS_SELECTOR, ".paginator")
                            if paginator:
                                # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥ï¼ˆåœ¨paginatorå†…çš„aæ ‡ç­¾ï¼‰
                                page_links = paginator.find_elements(By.CSS_SELECTOR, f"a[href*='users-users_fav'][href*='-page-']")
                                for link in page_links:
                                    href = link.get_attribute('href')
                                    # å¿…é¡»åŒ…å« users-users_fav å’Œ pageï¼Œä¸”æœªè®¿é—®è¿‡ï¼Œä¸”åŒ…å«å½“å‰category_id
                                    if href and href not in visited_urls and '-page-' in href and f'c-{category_id}' in href:
                                        candidate_urls.append(href)
                        except Exception as e:
                            pass
                        
                        if not candidate_urls:
                            try:
                                # æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†é¡µé“¾æ¥ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
                                all_page_links = self.driver.find_elements(By.CSS_SELECTOR, f"a[href*='users-users_fav'][href*='c-{category_id}'][href*='-page-']")
                                for link in all_page_links:
                                    href = link.get_attribute('href')
                                    if href and href not in visited_urls:
                                        candidate_urls.append(href)
                            except Exception as e:
                                pass
                        
                        # ä»å€™é€‰ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæœªè®¿é—®çš„URL
                        if candidate_urls:
                            next_page_url = candidate_urls[0]
                            # ğŸ”¥ ç¡®ä¿URLæ˜¯å®Œæ•´çš„ï¼ˆå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                            if next_page_url.startswith('/'):
                                base = self.base_url.rstrip('/')
                                next_page_url = f"{base}{next_page_url}"
                            elif not next_page_url.startswith('http'):
                                base = self.base_url.rstrip('/')
                                next_page_url = f"{base}/{next_page_url}"
                            logger.debug(f"    æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥: {next_page_url[:80]}")
                        
                        if not next_page_url:
                            logger.info(f"    æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        current_url = next_page_url
                        if not current_url:
                            logger.warning(f"    ä¸‹ä¸€é¡µé“¾æ¥æ— æ•ˆï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        page_num += 1
                        
                        if page_num > 100:
                            logger.warning(f"    å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶(100é¡µ)ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                    
                    logger.info(f"  {author} æ€»å…±è·å– {author_manga_count} ä¸ªæ¼«ç”»\n")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œç›´æ¥ä»å½“å‰é¡µé¢è·å–æ‰€æœ‰æ¼«ç”»
                logger.info("æœªæ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œä»å½“å‰é¡µé¢ç›´æ¥è·å–æ¼«ç”»...")
                manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                logger.info(f"æ‰¾åˆ° {len(manga_links)} ä¸ªæ¼«ç”»é“¾æ¥")
                
                for manga_link in manga_links:
                    try:
                        manga_url = manga_link.get_attribute('href')
                        title = manga_link.text.strip()
                        
                        if title and manga_url and manga_url not in manga_urls_set:
                            author = "æœªçŸ¥"
                            try:
                                parent = manga_link.find_element(By.XPATH, "./ancestor::*[position()<=5]")
                                author_elem = parent.find_elements(By.XPATH, ".//*[contains(@href, 'users-users_fav-c-')]")
                                if author_elem:
                                    author = author_elem[0].text.strip() or "æœªçŸ¥"
                            except:
                                pass
                            
                            page_count = None
                            try:
                                # æŸ¥æ‰¾åŒ…å«æ¼«ç”»é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œç„¶åæŸ¥æ‰¾ p.l_detla å…ƒç´ 
                                parent_container = manga_link.find_element(By.XPATH, "./ancestor::*[contains(@class, 'u_listcon') or contains(@class, 'box_cel')]")
                                page_elem = parent_container.find_element(By.CSS_SELECTOR, "p.l_detla")
                                if page_elem:
                                    page_text = page_elem.text
                                    # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                                    page_match = re.search(r'(\d+)\s*P?', page_text)
                                    if page_match:
                                        page_count = int(page_match.group(1))
                            except:
                                pass
                            
                            manga_urls_set.add(manga_url)
                            total_count += 1
                            
                            yield {
                                'title': title,
                                'author': author,
                                'manga_url': manga_url,
                                'page_count': page_count
                            }
                    except Exception as e:
                        logger.warning(f"å¤„ç†æ¼«ç”»å¤±è´¥: {e}")
                        continue
            
            logger.info(f"\nâœ“ æ”¶è—å¤¹çˆ¬å–å®Œæˆï¼Œæ€»å…± {total_count} ä¸ªæ¼«ç”»")
            
        except Exception as e:
            logger.error(f"è·å–æ”¶è—å¤¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

