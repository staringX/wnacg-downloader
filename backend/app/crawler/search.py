"""æœç´¢åŠŸèƒ½æ¨¡å—"""
import time
import re
from typing import List, Dict
from datetime import datetime
from urllib.parse import quote, urlparse, parse_qs, urlencode
from selenium.webdriver.common.by import By
from app.utils.logger import logger, get_error_message


class SearchCrawler:
    """æœç´¢çˆ¬å–å™¨ - è´Ÿè´£æœç´¢ä½œè€…å¹¶è·å–æ›´æ–°"""
    
    def __init__(self, browser_manager):
        self.browser = browser_manager
        self.driver = browser_manager.driver
    
    @property
    def base_url(self):
        """åŠ¨æ€è·å–base_urlï¼Œç¡®ä¿è·å–åˆ°æœ€æ–°å€¼"""
        return self.browser.base_url
    
    def search_author_updates(self, author_name: str, since_date: datetime) -> List[Dict]:
        """
        æœç´¢ä½œè€…å¹¶è·å–æ›´æ–°
        
        Args:
            author_name: ä½œè€…åç§°
            since_date: æˆªæ­¢æ—¥æœŸï¼Œåªè¿”å›æ™šäºæ­¤æ—¥æœŸçš„æ¼«ç”»
            
        Returns:
            æ¼«ç”»åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« title, manga_url, updated_at, page_count, cover_image_url, author
        """
        if not self.driver:
            logger.error("æµè§ˆå™¨é©±åŠ¨æœªåˆå§‹åŒ–")
            return []
        
        # ç¡®ä¿base_urlå·²è®¾ç½®
        if not self.base_url:
            logger.error("base_urlæœªè®¾ç½®ï¼Œæ— æ³•æœç´¢ä½œè€…æ›´æ–°")
            return []
        
        try:
            base = self.base_url.rstrip('/')
            # æ„é€ æœç´¢URLï¼ˆä½¿ç”¨æ­£ç¡®çš„æœç´¢æ ¼å¼ï¼‰
            encoded_author = quote(author_name)
            search_url = f"{base}/q/?q={encoded_author}&f=_all&s=create_time_DESC&syn=yes"
            logger.info(f"æœç´¢ä½œè€…: {author_name}, URL: {search_url}")
            
            visited_urls = set()
            current_url = search_url
            page_num = 1
            all_mangas = []
            
            # éå†æ‰€æœ‰æœç´¢ç»“æœé¡µé¢
            while True:
                if current_url in visited_urls:
                    logger.info(f"  æ£€æµ‹åˆ°é‡å¤URLï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                logger.info(f"  è®¿é—®ç¬¬ {page_num} é¡µ: {current_url}")
                self.driver.get(current_url)
                visited_urls.add(current_url)
                time.sleep(2)
                
                # æŸ¥æ‰¾æ¼«ç”»åˆ—è¡¨å®¹å™¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ï¼‰
                manga_info_list = []
                try:
                    # æŸ¥æ‰¾ ul.col_2 å®¹å™¨
                    container = self.driver.find_element(By.CSS_SELECTOR, "ul.col_2")
                    # æŸ¥æ‰¾æ‰€æœ‰ li.cate-* é¡¹
                    manga_items = container.find_elements(By.CSS_SELECTOR, "li[class*='cate-']")
                    logger.info(f"    æ‰¾åˆ° {len(manga_items)} ä¸ªæ¼«ç”»é¡¹")
                    
                    for item in manga_items:
                        try:
                            # æŸ¥æ‰¾æ¼«ç”»é“¾æ¥
                            manga_link = item.find_element(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                            manga_url = manga_link.get_attribute('href')
                            title = manga_link.text.strip()
                            
                            if not manga_url or not title:
                                continue
                            
                            # ç¡®ä¿URLå®Œæ•´
                            if manga_url.startswith('/'):
                                manga_url = f"{base}{manga_url}"
                            elif not manga_url.startswith('http'):
                                manga_url = f"{base}/{manga_url}"
                            
                            # è·å–åˆ›å»ºæ—¶é—´å’Œé¡µæ•°ï¼ˆä» span.info ä¸­æå–ï¼‰
                            updated_at = None
                            page_count = None
                            try:
                                info_span = item.find_element(By.CSS_SELECTOR, "span.info")
                                info_text = info_span.text
                                
                                # æå–æ—¥æœŸï¼šåˆ›å»ºäº2024-09-21 01:45:25
                                date_match = re.search(r'åˆ›å»ºäº(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2})', info_text)
                                if date_match:
                                    date_str = f"{date_match.group(1)} {date_match.group(2)}"
                                    updated_at = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
                                else:
                                    # å°è¯•åªæå–æ—¥æœŸ
                                    date_match = re.search(r'åˆ›å»ºäº(\d{4}-\d{2}-\d{2})', info_text)
                                    if date_match:
                                        date_str = date_match.group(1)
                                        updated_at = datetime.strptime(date_str, '%Y-%m-%d')
                                
                                # æå–é¡µæ•°ï¼š55å¼ å›¾ç‰‡
                                page_match = re.search(r'(\d+)å¼ å›¾ç‰‡', info_text)
                                if page_match:
                                    page_count = int(page_match.group(1))
                            except:
                                pass
                            
                            # è·å–å°é¢å›¾ç‰‡URL
                            cover_image_url = None
                            try:
                                img = item.find_element(By.CSS_SELECTOR, "img[src*='wnimg'], img[src*='qy0']")
                                cover_image_url = img.get_attribute('src')
                                if cover_image_url:
                                    if cover_image_url.startswith('//'):
                                        cover_image_url = f"https:{cover_image_url}"
                                    elif cover_image_url.startswith('/'):
                                        cover_image_url = f"{base}{cover_image_url}"
                                    elif not cover_image_url.startswith('http'):
                                        cover_image_url = f"{base}/{cover_image_url}"
                            except:
                                pass
                            
                            manga_info_list.append({
                                'url': manga_url,
                                'title': title,
                                'updated_at': updated_at,
                                'page_count': page_count,
                                'cover_image_url': cover_image_url
                            })
                        except Exception as e:
                            logger.warning(f"    æå–æ¼«ç”»ä¿¡æ¯å¤±è´¥: {get_error_message(e)}")
                            continue
                except Exception as e:
                    logger.warning(f"    æŸ¥æ‰¾æ¼«ç”»åˆ—è¡¨å¤±è´¥: {get_error_message(e)}")
                    # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰æ¼«ç”»é“¾æ¥
                    manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                    logger.info(f"    å¤‡ç”¨æ–¹æ³•æ‰¾åˆ° {len(manga_links)} ä¸ªæ¼«ç”»é“¾æ¥")
                    for manga_link in manga_links:
                        try:
                            manga_url = manga_link.get_attribute('href')
                            title = manga_link.text.strip()
                            if manga_url and title:
                                if manga_url.startswith('/'):
                                    manga_url = f"{base}{manga_url}"
                                manga_info_list.append({
                                    'url': manga_url,
                                    'title': title,
                                    'updated_at': None,
                                    'page_count': None,
                                    'cover_image_url': None
                                })
                        except:
                            continue
                
                if len(manga_info_list) == 0:
                    logger.info(f"    ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°æ¼«ç”»ï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                # ç­›é€‰å‡ºæ™šäºæˆªæ­¢æ—¥æœŸçš„æ¼«ç”»ï¼ŒåŒæ—¶æ£€æŸ¥æ˜¯å¦é‡åˆ°æ—©äºæˆªæ­¢æ—¥æœŸçš„æ¼«ç”»
                # ç”±äºæœç´¢ç»“æœæŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼Œä¸€æ—¦é‡åˆ°æ—©äºæˆªæ­¢æ—¥æœŸçš„æ¼«ç”»ï¼Œåé¢çš„éƒ½æ˜¯æ›´æ—§çš„ï¼Œå¯ä»¥åœæ­¢ç¿»é¡µ
                should_stop = False
                for manga_info in manga_info_list:
                    if manga_info['updated_at']:
                        if manga_info['updated_at'] > since_date:
                            # æ™šäºæˆªæ­¢æ—¥æœŸï¼Œæ·»åŠ åˆ°ç»“æœ
                            all_mangas.append({
                                'title': manga_info['title'],
                                'manga_url': manga_info['url'],
                                'updated_at': manga_info['updated_at'],
                                'page_count': manga_info['page_count'],
                                'cover_image_url': manga_info['cover_image_url'],
                                'author': author_name
                            })
                        else:
                            # æ—©äºæˆ–ç­‰äºæˆªæ­¢æ—¥æœŸï¼Œç”±äºç»“æœæŒ‰æ—¶é—´å€’åºï¼Œåé¢çš„éƒ½æ˜¯æ›´æ—§çš„ï¼Œå¯ä»¥åœæ­¢ç¿»é¡µ
                            logger.info(f"    é‡åˆ°æ—©äºæˆªæ­¢æ—¥æœŸçš„æ¼«ç”»ï¼ˆ{manga_info['updated_at']} <= {since_date}ï¼‰ï¼Œåœæ­¢ç¿»é¡µ")
                            should_stop = True
                            break
                
                # å¦‚æœé‡åˆ°æ—©äºæˆªæ­¢æ—¥æœŸçš„æ¼«ç”»ï¼Œåœæ­¢ç¿»é¡µ
                if should_stop:
                    break
                
                # ğŸ”¥ æœç´¢ç»“æœé¡µåªæœ‰æ•°å­—åˆ†é¡µï¼Œé€šè¿‡è®°å½•å½“å‰é¡µç ï¼ŒæŸ¥æ‰¾é¡µç ä¸º"å½“å‰é¡µ+1"çš„é“¾æ¥
                next_page_url = None
                try:
                    # æ ¹æ®MCPç¡®è®¤çš„ç»“æ„ï¼šåˆ†é¡µå™¨æœ‰class "paginator"
                    paginator = self.driver.find_element(By.CSS_SELECTOR, ".paginator")
                    if paginator:
                        # è·å–å½“å‰é¡µé¡µç 
                        current_page_num = page_num  # ä½¿ç”¨ä¸´æ—¶å˜é‡page_numä½œä¸ºå½“å‰é¡µ
                        try:
                            # å°è¯•ä».thispageå…ƒç´ è·å–å½“å‰é¡µï¼ˆæ›´å‡†ç¡®ï¼‰
                            thispage_elem = paginator.find_element(By.CSS_SELECTOR, ".thispage")
                            if thispage_elem:
                                current_page_num = int(thispage_elem.text.strip())
                        except:
                            # å¦‚æœæ‰¾ä¸åˆ°.thispageï¼Œä½¿ç”¨page_numå˜é‡
                            pass
                        
                        # è®¡ç®—ä¸‹ä¸€é¡µé¡µç 
                        next_page_num = current_page_num + 1
                        logger.debug(f"    å½“å‰é¡µ: {current_page_num}, æŸ¥æ‰¾é¡µç ä¸º {next_page_num} çš„é“¾æ¥")
                        
                        # åœ¨åˆ†é¡µå™¨ä¸­æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼Œæ‰¾åˆ°é¡µç ç­‰äº"å½“å‰é¡µ+1"çš„é“¾æ¥
                        page_links = paginator.find_elements(By.CSS_SELECTOR, "a")
                        
                        for link in page_links:
                            href = link.get_attribute('href')
                            if not href:
                                continue
                            
                            # å¤„ç†ç›¸å¯¹è·¯å¾„
                            if href.startswith('/'):
                                full_url = f"{base}{href}"
                            elif not href.startswith('http'):
                                full_url = f"{base}/{href}"
                            else:
                                full_url = href
                            
                            # ä»URLä¸­æå–é¡µç å‚æ•°ï¼ˆæ ¼å¼ï¼šp=2 æˆ– &p=2ï¼‰
                            page_match = re.search(r'[&?]p=(\d+)', full_url)
                            if page_match:
                                link_page_num = int(page_match.group(1))
                                # æ‰¾åˆ°é¡µç ç­‰äº"å½“å‰é¡µ+1"ä¸”æœªè®¿é—®è¿‡çš„é“¾æ¥
                                if (link_page_num == next_page_num and 
                                    full_url not in visited_urls and
                                    'q=' in full_url):
                                    next_page_url = full_url
                                    logger.info(f"    âœ“ æ‰¾åˆ°é¡µç ä¸º {next_page_num} çš„é“¾æ¥: {next_page_url[:80]}")
                                    break
                except Exception as e:
                    logger.debug(f"    æŸ¥æ‰¾åˆ†é¡µå™¨å¤±è´¥: {get_error_message(e)}")
                    pass
                
                # å¦‚æœæ‰¾ä¸åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œè¯´æ˜å·²ç»åˆ°æœ€åä¸€é¡µï¼Œéå†å®Œå½“å‰é¡µåç»“æŸ
                if not next_page_url:
                    logger.info(f"    âš ï¸  æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œè¿™æ˜¯æœ€åä¸€é¡µï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                # next_page_urlå·²ç»åœ¨å‰é¢å¤„ç†è¿‡ï¼Œç›´æ¥ä½¿ç”¨
                current_url = next_page_url
                page_num += 1
                
                if page_num > 100:  # é™åˆ¶æœ€å¤§é¡µæ•°
                    logger.warning(f"    å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶(100é¡µ)ï¼Œåœæ­¢ç¿»é¡µ")
                    break
            
            logger.info(f"  ä½œè€… {author_name} å…±æ‰¾åˆ° {len(all_mangas)} ä¸ªæ–°æ›´æ–°")
            return all_mangas
            
        except Exception as e:
            logger.error(f"æœç´¢ä½œè€… {author_name} å¤±è´¥: {get_error_message(e)}")
            return []

