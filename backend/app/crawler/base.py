import requests
from bs4 import BeautifulSoup
from typing import List, Optional, Dict
import time
import re
from datetime import datetime
from app.config import settings
from app.utils.logger import logger

# å¯é€‰çš„Seleniumå¯¼å…¥
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("è­¦å‘Š: Seleniumæœªå®‰è£…ï¼Œçˆ¬è™«åŠŸèƒ½å°†ä¸å¯ç”¨")


class MangaCrawler:
    def __init__(self):
        self.base_url: Optional[str] = None
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.driver: Optional[webdriver.Chrome] = None
        self._init_driver()
    
    def _init_driver(self):
        """åˆå§‹åŒ–Chromeæµè§ˆå™¨é©±åŠ¨"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # åœ¨Dockerç¯å¢ƒä¸­ï¼Œchromedriverå¯èƒ½åœ¨/usr/local/bin/chromedriveræˆ–/usr/bin/chromedriver
        import os
        chromedriver_paths = [
            '/usr/local/bin/chromedriver',
            '/usr/bin/chromedriver',
            '/usr/bin/chromium-driver'
        ]
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Chromiumï¼ˆDockerç¯å¢ƒï¼‰
        chromium_binary_paths = [
            '/usr/bin/chromium',
            '/usr/bin/chromium-browser'
        ]
        
        chromedriver_path = None
        for path in chromedriver_paths:
            if os.path.exists(path):
                chromedriver_path = path
                break
        
        # å¦‚æœæ‰¾åˆ°Chromiumï¼Œè®¾ç½®binaryè·¯å¾„
        for chromium_path in chromium_binary_paths:
            if os.path.exists(chromium_path):
                chrome_options.binary_location = chromium_path
                break
        
        try:
            if chromedriver_path:
                service = Service(chromedriver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹
                self.driver = webdriver.Chrome(options=chrome_options)
        except Exception as e:
            logger.error(f"æ— æ³•åˆå§‹åŒ–Chromeé©±åŠ¨: {e}")
            self.driver = None
    
    def get_available_url(self) -> Optional[str]:
        """ä»å‘å¸ƒé¡µè·å–å¯ç”¨çš„æ¼«ç”»ç½‘ç«™åœ°å€"""
        try:
            response = self.session.get(settings.publish_page_url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = soup.find_all('a', href=True)
            urls = []
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # åŒ¹é…ç½‘ç«™åœ°å€æ¨¡å¼ï¼ˆwww.wn*.ru æˆ– www.wnacg*.ccç­‰ï¼‰
                if re.match(r'https?://www\.wn\d+\.ru', href) or \
                   re.match(r'https?://www\.wnacg\d+\.cc', href):
                    urls.append(href)
            
            # å°è¯•è¿æ¥æ¯ä¸ªURLï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„
            for url in urls:
                try:
                    test_response = self.session.get(f"{url}/", timeout=5)
                    if test_response.status_code == 200:
                        return url
                except:
                    continue
            
            return None
        except Exception as e:
            print(f"è·å–ç½‘ç«™åœ°å€å¤±è´¥: {e}")
            return None
    
    def login(self, username: str, password: str) -> bool:
        """ç™»å½•ç½‘ç«™"""
        if not self.base_url:
            self.base_url = self.get_available_url()
            if not self.base_url:
                return False
        
        if not self.driver:
            return False
        
        try:
            # å¯¼èˆªåˆ°ç™»å½•é¡µé¢
            base = self.base_url.rstrip('/')
            login_url = f"{base}/users-login.html"
            self.driver.get(login_url)
            time.sleep(3)
            
            # æŸ¥æ‰¾å¹¶å¡«å†™ç™»å½•è¡¨å•
            username_input = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.NAME, "login_name"))
            )
            password_input = self.driver.find_element(By.NAME, "login_pass")
            
            username_input.clear()
            username_input.send_keys(username)
            password_input.clear()
            password_input.send_keys(password)
            
            # ç‚¹å‡»ç™»å½•æŒ‰é’®
            login_button = self.driver.find_element(By.CSS_SELECTOR, "button, input[type='submit']")
            login_button.click()
            
            # ç­‰å¾…ç™»å½•å®Œæˆ
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸï¼ˆæŸ¥æ‰¾ç”¨æˆ·åæˆ–é€€å‡ºç™»å½•é“¾æ¥ï¼‰
            current_url = self.driver.current_url
            print(f"ç™»å½•åè·³è½¬åˆ°: {current_url}")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            time.sleep(2)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«ç™»å½•æˆåŠŸçš„æ ‡å¿—
            page_source = self.driver.page_source
            if "users-login" not in current_url or "æˆ‘çš„ç©ºé–“" in page_source or "lilifan456" in page_source:
                # è·å–cookieså¹¶è®¾ç½®åˆ°session
                cookies = self.driver.get_cookies()
                for cookie in cookies:
                    self.session.cookies.set(cookie['name'], cookie['value'])
                print("ç™»å½•æˆåŠŸï¼Œå·²ä¿å­˜cookies")
                return True
            
            print("ç™»å½•å¯èƒ½å¤±è´¥ï¼Œä»åœ¨ç™»å½•é¡µé¢")
            return False
        except Exception as e:
            print(f"ç™»å½•å¤±è´¥: {e}")
            return False
    
    def get_collection(self) -> List[Dict]:
        """è·å–æ”¶è—å¤¹ä¸­çš„æ‰€æœ‰æ¼«ç”»ï¼ŒæŒ‰ä½œè€…åˆ†ç±»"""
        if not self.driver:
            return []
        
        try:
            # é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§é¡µé¢æ˜¾ç¤ºé¡ºåºæ”¶é›†æ¼«ç”»ï¼Œä¸èƒ½å¯¹åˆ—è¡¨è¿›è¡Œä»»ä½•æ’åºï¼
            mangas = []  # ä¿æŒé¡µé¢é¡ºåºçš„æ¼«ç”»åˆ—è¡¨
            manga_urls_set = set()  # ç”¨äºå¿«é€Ÿå»é‡
            base = self.base_url.rstrip('/')
            
            # æ­£ç¡®çš„ä¹¦æ¶URLï¼ˆé€šè¿‡Chrome DevTools MCPç¡®è®¤ï¼‰
            bookshelf_url = f"{base}/users-users_fav.html"
            print(f"è®¿é—®ä¹¦æ¶é¡µé¢: {bookshelf_url}")
            self.driver.get(bookshelf_url)
            time.sleep(5)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æˆåŠŸåŠ è½½
            current_url = self.driver.current_url
            page_title = self.driver.title
            print(f"å½“å‰é¡µé¢URL: {current_url}")
            print(f"é¡µé¢æ ‡é¢˜: {page_title}")
            
            if "404" in page_title.lower() or "404" in self.driver.page_source[:1000].lower():
                print(f"ä¹¦æ¶é¡µé¢è¿”å›404")
                return []
            
            # æŸ¥æ‰¾åˆ†ç±»é“¾æ¥ï¼ˆæ ¼å¼ï¼šusers-users_fav-c-{id}.htmlï¼‰
            category_links = {}  # {author_name: link_url}
            all_links = self.driver.find_elements(By.CSS_SELECTOR, "a")
            
            for link in all_links:
                href = link.get_attribute('href') or ''
                text = link.text.strip()
                
                # è¯†åˆ«åˆ†ç±»é“¾æ¥ï¼šusers-users_fav-c-{id}.html
                if 'users-users_fav-c-' in href and text:
                    # æ’é™¤"å…¨éƒ¨"å’Œ"ç®¡ç†åˆ†é¡"
                    if text not in ["å…¨éƒ¨", "ç®¡ç†åˆ†é¡", "æ›¸æ¶", "ä¹¦æ¶", "æˆ‘çš„æ›¸æ¶"]:
                        category_links[text] = href
                        print(f"æ‰¾åˆ°åˆ†ç±»: {text} -> {href}")
            
            print(f"å…±æ‰¾åˆ° {len(category_links)} ä¸ªä½œè€…åˆ†ç±»")
            
            # å¦‚æœæœ‰åˆ†ç±»ï¼ŒæŒ‰åˆ†ç±»è·å–æ¼«ç”»
            if category_links:
                for author, category_url in category_links.items():
                    print(f"\nå¤„ç†ä½œè€…åˆ†ç±»: {author}")
                    
                    # æå–åˆ†ç±»IDï¼ˆä»URLä¸­ï¼‰
                    # æ ¼å¼ï¼šusers-users_fav-c-{category_id}.html
                    category_id_match = re.search(r'users-users_fav-c-(\d+)\.html', category_url)
                    if not category_id_match:
                        print(f"  æ— æ³•æå–åˆ†ç±»IDï¼Œè·³è¿‡")
                        continue
                    
                    category_id = category_id_match.group(1)
                    page_num = 1
                    author_manga_count = 0
                    
                    # ä»ç¬¬ä¸€é¡µå¼€å§‹
                    current_url = category_url
                    visited_urls = set()
                    
                    # éå†æ‰€æœ‰åˆ†é¡µ
                    while True:
                        # é¿å…é‡å¤è®¿é—®
                        if current_url in visited_urls:
                            print(f"  æ£€æµ‹åˆ°é‡å¤URLï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        print(f"  è®¿é—®ç¬¬ {page_num} é¡µ: {current_url}")
                        self.driver.get(current_url)
                        visited_urls.add(current_url)
                        time.sleep(2)
                        
                        # æŸ¥æ‰¾è¯¥é¡µé¢ä¸‹çš„æ‰€æœ‰æ¼«ç”»é“¾æ¥
                        manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                        page_manga_count = 0
                        
                        # æŒ‰ç…§é¡µé¢é¡ºåºå¤„ç†æ¼«ç”»é“¾æ¥
                        for manga_link in manga_links:
                            try:
                                manga_url = manga_link.get_attribute('href')
                                title = manga_link.text.strip()
                                
                                # è·³è¿‡ç©ºæ ‡é¢˜æˆ–ç©ºé“¾æ¥
                                if not title or not manga_url:
                                    continue
                                
                                # å¿«é€Ÿå»é‡ï¼šä½¿ç”¨ set æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ 
                                if manga_url in manga_urls_set:
                                    continue
                                
                                # å°è¯•è·å–é¡µæ•°ä¿¡æ¯ï¼ˆä½¿ç”¨classåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                                page_count = None
                                try:
                                    # æŸ¥æ‰¾åŒ…å«æ¼«ç”»é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œç„¶åæŸ¥æ‰¾ p.l_detla å…ƒç´ 
                                    parent_container = manga_link.find_element(By.XPATH, "./ancestor::*[contains(@class, 'u_listcon') or contains(@class, 'box_cel')]")
                                    page_elem = parent_container.find_element(By.CSS_SELECTOR, "p.l_detla")
                                    if page_elem:
                                        page_text = page_elem.text
                                        # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                                        page_match = re.search(r'(\d+)\s*P?', page_text)
                                        if page_match:
                                            page_count = int(page_match.group(1))
                                except:
                                    pass
                                
                                # æŒ‰ç…§é¡µé¢é¡ºåºæ·»åŠ åˆ°åˆ—è¡¨
                                mangas.append({
                                    'title': title,
                                    'author': author,
                                    'manga_url': manga_url,
                                    'page_count': page_count
                                })
                                manga_urls_set.add(manga_url)  # æ·»åŠ åˆ° set ç”¨äºå»é‡
                                page_manga_count += 1
                                author_manga_count += 1
                            except Exception as e:
                                print(f"    å¤„ç†æ¼«ç”»å¤±è´¥: {e}")
                                continue
                        
                        print(f"    ç¬¬ {page_num} é¡µï¼šæ‰¾åˆ° {page_manga_count} ä¸ªæ¼«ç”»")
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¼«ç”»ï¼Œåœæ­¢ç¿»é¡µ
                        if page_manga_count == 0:
                            print(f"    ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°æ¼«ç”»ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆä½¿ç”¨HTMLå…ƒç´ å’ŒClassåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                        next_page_url = None
                        try:
                            # é€šè¿‡åˆ†é¡µå™¨ç»“æ„æŸ¥æ‰¾ï¼ˆä½¿ç”¨classåç§°ï¼‰
                            paginator = self.driver.find_element(By.CSS_SELECTOR, ".paginator")
                            if paginator:
                                # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥ï¼ˆåœ¨paginatorå†…çš„aæ ‡ç­¾ï¼‰
                                page_links = paginator.find_elements(By.CSS_SELECTOR, f"a[href*='users-users_fav'][href*='c-{category_id}'][href*='-page-']")
                                for link in page_links:
                                    href = link.get_attribute('href')
                                    if href and href not in visited_urls:
                                        next_page_url = href
                                        print(f"    æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆURLæ¨¡å¼åŒ¹é…ï¼‰")
                                        break
                        except Exception as e:
                            print(f"    æŸ¥æ‰¾ä¸‹ä¸€é¡µå¤±è´¥: {e}")
                        
                        if not next_page_url:
                            print(f"    æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„ï¼ˆå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                        if next_page_url.startswith('/'):
                            base = self.base_url.rstrip('/')
                            next_page_url = f"{base}{next_page_url}"
                        elif not next_page_url.startswith('http'):
                            base = self.base_url.rstrip('/')
                            next_page_url = f"{base}/{next_page_url}"
                        
                        current_url = next_page_url
                        if not current_url:
                            print(f"    ä¸‹ä¸€é¡µé“¾æ¥æ— æ•ˆï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        page_num += 1
                        
                        # å®‰å…¨æ£€æŸ¥ï¼šæœ€å¤šç¿»100é¡µ
                        if page_num > 100:
                            print(f"    å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶(100é¡µ)ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                    
                    print(f"  {author} æ€»å…±è·å– {author_manga_count} ä¸ªæ¼«ç”»")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œç›´æ¥ä»å½“å‰é¡µé¢è·å–æ‰€æœ‰æ¼«ç”»
                print("æœªæ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œä»å½“å‰é¡µé¢ç›´æ¥è·å–æ¼«ç”»...")
                manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                print(f"æ‰¾åˆ° {len(manga_links)} ä¸ªæ¼«ç”»é“¾æ¥")
                
                for manga_link in manga_links:
                    try:
                        manga_url = manga_link.get_attribute('href')
                        title = manga_link.text.strip()
                        
                        if title and manga_url:
                            # å°è¯•ä»é¡µé¢ä¸­æå–ä½œè€…ä¿¡æ¯
                            author = "æœªçŸ¥"
                            try:
                                # æŸ¥æ‰¾åˆ†ç±»åç§°ï¼ˆé€šå¸¸åœ¨é“¾æ¥é™„è¿‘ï¼‰
                                parent = manga_link.find_element(By.XPATH, "./ancestor::*[position()<=5]")
                                author_elem = parent.find_elements(By.XPATH, ".//*[contains(@href, 'users-users_fav-c-')]")
                                if author_elem:
                                    author = author_elem[0].text.strip() or "æœªçŸ¥"
                            except:
                                pass
                            
                            # è·å–é¡µæ•°ï¼ˆä½¿ç”¨classåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                            page_count = None
                            try:
                                # æŸ¥æ‰¾åŒ…å«æ¼«ç”»é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œç„¶åæŸ¥æ‰¾ p.l_detla å…ƒç´ 
                                parent_container = manga_link.find_element(By.XPATH, "./ancestor::*[contains(@class, 'u_listcon') or contains(@class, 'box_cel')]")
                                page_elem = parent_container.find_element(By.CSS_SELECTOR, "p.l_detla")
                                if page_elem:
                                    page_text = page_elem.text
                                    # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                                    page_match = re.search(r'(\d+)\s*P?', page_text)
                                    if page_match:
                                        page_count = int(page_match.group(1))
                            except:
                                pass
                            
                            mangas.append({
                                'title': title,
                                'author': author,
                                'manga_url': manga_url,
                                'page_count': page_count
                            })
                    except Exception as e:
                        print(f"å¤„ç†æ¼«ç”»å¤±è´¥: {e}")
                        continue
            
            print(f"\næ€»å…±è·å–åˆ° {len(mangas)} ä¸ªæ¼«ç”»")
            return mangas
        except Exception as e:
            print(f"è·å–æ”¶è—å¤¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_collection_stream(self):
        """
        è·å–æ”¶è—å¤¹ä¸­çš„æ‰€æœ‰æ¼«ç”»ï¼ˆç”Ÿæˆå™¨ç‰ˆæœ¬ï¼‰
        è¾¹çˆ¬å–è¾¹è¿”å›ï¼Œä¸ç­‰å¾…å…¨éƒ¨å®Œæˆï¼Œå®ç°çœŸæ­£çš„å®æ—¶åŒæ­¥
        
        ä¼˜åŠ¿ï¼š
        1. æå¿«çš„é¦–æ¬¡å“åº”ï¼ˆ2-5ç§’å°±èƒ½çœ‹åˆ°ç¬¬ä¸€æ‰¹æ¼«ç”»ï¼‰
        2. å†…å­˜æ•ˆç‡é«˜ï¼ˆä¸éœ€è¦å­˜å‚¨å®Œæ•´åˆ—è¡¨ï¼‰
        3. ä»»ä½•æ—¶å€™ä¸­æ–­éƒ½ä¸ä¼šä¸¢å¤±å·²å¤„ç†çš„æ•°æ®
        4. ç”¨æˆ·å¯ä»¥å®æ—¶çœ‹åˆ°åŒæ­¥è¿›åº¦
        
        Yields:
            dict: æ¼«ç”»ä¿¡æ¯å­—å…¸ {'title', 'author', 'manga_url', 'page_count'}
        """
        if not self.driver:
            return
        
        try:
            manga_urls_set = set()  # ç”¨äºå»é‡
            base = self.base_url.rstrip('/')
            
            # æ­£ç¡®çš„ä¹¦æ¶URL
            bookshelf_url = f"{base}/users-users_fav.html"
            print(f"è®¿é—®ä¹¦æ¶é¡µé¢: {bookshelf_url}")
            self.driver.get(bookshelf_url)
            time.sleep(5)
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æˆåŠŸåŠ è½½
            current_url = self.driver.current_url
            page_title = self.driver.title
            print(f"å½“å‰é¡µé¢URL: {current_url}")
            print(f"é¡µé¢æ ‡é¢˜: {page_title}")
            
            if "404" in page_title.lower() or "404" in self.driver.page_source[:1000].lower():
                print(f"ä¹¦æ¶é¡µé¢è¿”å›404")
                return
            
            # æŸ¥æ‰¾åˆ†ç±»é“¾æ¥
            category_links = {}
            all_links = self.driver.find_elements(By.CSS_SELECTOR, "a")
            
            for link in all_links:
                href = link.get_attribute('href') or ''
                text = link.text.strip()
                
                if 'users-users_fav-c-' in href and text:
                    if text not in ["å…¨éƒ¨", "ç®¡ç†åˆ†é¡", "æ›¸æ¶", "ä¹¦æ¶", "æˆ‘çš„æ›¸æ¶"]:
                        category_links[text] = href
                        print(f"æ‰¾åˆ°åˆ†ç±»: {text} -> {href}")
            
            print(f"å…±æ‰¾åˆ° {len(category_links)} ä¸ªä½œè€…åˆ†ç±»\n")
            
            total_count = 0
            
            # å¦‚æœæœ‰åˆ†ç±»ï¼ŒæŒ‰åˆ†ç±»è·å–æ¼«ç”»
            if category_links:
                for author_idx, (author, category_url) in enumerate(category_links.items(), 1):
                    print(f"[{author_idx}/{len(category_links)}] å¤„ç†ä½œè€…åˆ†ç±»: {author}")
                    
                    # æå–åˆ†ç±»ID
                    category_id_match = re.search(r'users-users_fav-c-(\d+)\.html', category_url)
                    if not category_id_match:
                        print(f"  æ— æ³•æå–åˆ†ç±»IDï¼Œè·³è¿‡")
                        continue
                    
                    category_id = category_id_match.group(1)
                    page_num = 1
                    author_manga_count = 0
                    
                    current_url = category_url
                    visited_urls = set()
                    
                    # éå†æ‰€æœ‰åˆ†é¡µ
                    while True:
                        if current_url in visited_urls:
                            print(f"  æ£€æµ‹åˆ°é‡å¤URLï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        print(f"  è®¿é—®ç¬¬ {page_num} é¡µ: {current_url}")
                        self.driver.get(current_url)
                        visited_urls.add(current_url)
                        time.sleep(2)
                        
                        # æŸ¥æ‰¾è¯¥é¡µé¢ä¸‹çš„æ‰€æœ‰æ¼«ç”»é“¾æ¥
                        manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                        print(f"    ğŸ” CSSæ‰¾åˆ° {len(manga_links)} ä¸ªé“¾æ¥")
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç«‹å³æå–æ‰€æœ‰é“¾æ¥ä¿¡æ¯ï¼Œé¿å…stale element reference
                        manga_info_list = []
                        for manga_link in manga_links:
                            try:
                                manga_url = manga_link.get_attribute('href')
                                title = manga_link.text.strip()
                                
                                # å°è¯•è·å–é¡µæ•°ä¿¡æ¯ï¼ˆä½¿ç”¨classåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                                page_count = None
                                try:
                                    # æŸ¥æ‰¾åŒ…å«æ¼«ç”»é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œç„¶åæŸ¥æ‰¾ p.l_detla å…ƒç´ 
                                    parent_container = manga_link.find_element(By.XPATH, "./ancestor::*[contains(@class, 'u_listcon') or contains(@class, 'box_cel')]")
                                    page_elem = parent_container.find_element(By.CSS_SELECTOR, "p.l_detla")
                                    if page_elem:
                                        page_text = page_elem.text
                                        # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                                        page_match = re.search(r'(\d+)\s*P?', page_text)
                                        if page_match:
                                            page_count = int(page_match.group(1))
                                except:
                                    pass
                                
                                if manga_url and title:
                                    manga_info_list.append({
                                        'url': manga_url,
                                        'title': title,
                                        'page_count': page_count
                                    })
                            except Exception as e:
                                # å¦‚æœè·å–ä¿¡æ¯å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªé“¾æ¥
                                continue
                        
                        # ç°åœ¨å¤„ç†æå–çš„ä¿¡æ¯åˆ—è¡¨
                        page_manga_count = 0
                        empty_count = 0
                        dup_count = 0
                        
                        for idx, manga_info in enumerate(manga_info_list, 1):
                            try:
                                manga_url = manga_info['url']
                                title = manga_info['title']
                                page_count = manga_info.get('page_count')
                                
                                if idx <= 3:  # æ‰“å°å‰3ä¸ª
                                    print(f"      [{idx}] URL={manga_url[-30:]}, Title='{title[:50]}'")
                                
                                if not title or not manga_url:
                                    empty_count += 1
                                    if idx <= 3:
                                        print(f"      [{idx}] âŒ è·³è¿‡ï¼šæ ‡é¢˜æˆ–URLä¸ºç©º")
                                    continue
                                
                                # å»é‡
                                if manga_url in manga_urls_set:
                                    dup_count += 1
                                    if idx <= 3:
                                        print(f"      [{idx}] â­ï¸  è·³è¿‡ï¼šé‡å¤")
                                    continue
                                
                                # âœ¨ å…³é”®ï¼šç«‹å³ yieldï¼Œä¸ç­‰å¾…åç»­çˆ¬å–
                                manga_urls_set.add(manga_url)
                                page_manga_count += 1
                                author_manga_count += 1
                                total_count += 1
                                
                                yield {
                                    'title': title,
                                    'author': author,
                                    'manga_url': manga_url,
                                    'page_count': page_count
                                }
                                
                            except Exception as e:
                                print(f"    å¤„ç†æ¼«ç”»å¤±è´¥: {e}")
                                continue
                        
                        print(f"    ç¬¬ {page_num} é¡µï¼šæ‰¾åˆ° {page_manga_count} ä¸ªæ¼«ç”»ï¼ˆæ€»è®¡: {total_count}ï¼‰")
                        print(f"    ğŸ“Š è·³è¿‡ï¼šç©ºæ ‡é¢˜/URL={empty_count}, é‡å¤={dup_count}")
                        
                        if page_manga_count == 0:
                            print(f"    ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°æ¼«ç”»ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆä½¿ç”¨HTMLå…ƒç´ å’ŒClassåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                        next_page_url = None
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆæ”¶é›†æ‰€æœ‰å¯èƒ½çš„ç¿»é¡µé“¾æ¥URLï¼Œé¿å…stale element reference
                        candidate_urls = []
                        
                        try:
                            # æ–¹æ³•1ï¼šé€šè¿‡åˆ†é¡µå™¨ç»“æ„æŸ¥æ‰¾ï¼ˆä½¿ç”¨classåç§°ï¼‰
                            # æŸ¥æ‰¾ .paginator ä¸­çš„é“¾æ¥ï¼Œå½“å‰é¡µæ˜¯ span.thispageï¼Œä¸‹ä¸€é¡µæ˜¯ä¸‹ä¸€ä¸ª a æ ‡ç­¾
                            paginator = self.driver.find_element(By.CSS_SELECTOR, ".paginator")
                            if paginator:
                                # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥ï¼ˆåœ¨paginatorå†…çš„aæ ‡ç­¾ï¼‰
                                page_links = paginator.find_elements(By.CSS_SELECTOR, "a[href*='users-users_fav'][href*='-page-']")
                                for link in page_links:
                                    href = link.get_attribute('href')
                                    # å¿…é¡»åŒ…å« users-users_fav å’Œ pageï¼Œä¸”æœªè®¿é—®è¿‡ï¼Œä¸”åŒ…å«å½“å‰category_id
                                    if href and href not in visited_urls and '-page-' in href and f'c-{category_id}' in href:
                                        candidate_urls.append(href)
                        except Exception as e:
                            pass
                        
                        if not candidate_urls:
                            try:
                                # æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†é¡µé“¾æ¥ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
                                all_page_links = self.driver.find_elements(By.CSS_SELECTOR, f"a[href*='users-users_fav'][href*='c-{category_id}'][href*='-page-']")
                                for link in all_page_links:
                                    href = link.get_attribute('href')
                                    if href and href not in visited_urls:
                                        candidate_urls.append(href)
                            except Exception as e:
                                pass
                        
                        # ä»å€™é€‰ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæœªè®¿é—®çš„URL
                        if candidate_urls:
                            next_page_url = candidate_urls[0]
                            # ğŸ”¥ ç¡®ä¿URLæ˜¯å®Œæ•´çš„ï¼ˆå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                            if next_page_url.startswith('/'):
                                base = self.base_url.rstrip('/')
                                next_page_url = f"{base}{next_page_url}"
                            elif not next_page_url.startswith('http'):
                                base = self.base_url.rstrip('/')
                                next_page_url = f"{base}/{next_page_url}"
                            print(f"    æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥: {next_page_url[:80]}")
                        
                        if not next_page_url:
                            print(f"    æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        current_url = next_page_url
                        if not current_url:
                            print(f"    ä¸‹ä¸€é¡µé“¾æ¥æ— æ•ˆï¼Œåœæ­¢ç¿»é¡µ")
                            break
                        
                        page_num += 1
                        
                        if page_num > 100:
                            print(f"    å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶(100é¡µ)ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                    
                    print(f"  {author} æ€»å…±è·å– {author_manga_count} ä¸ªæ¼«ç”»\n")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œç›´æ¥ä»å½“å‰é¡µé¢è·å–æ‰€æœ‰æ¼«ç”»
                print("æœªæ‰¾åˆ°åˆ†ç±»é“¾æ¥ï¼Œä»å½“å‰é¡µé¢ç›´æ¥è·å–æ¼«ç”»...")
                manga_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index-aid-']")
                print(f"æ‰¾åˆ° {len(manga_links)} ä¸ªæ¼«ç”»é“¾æ¥")
                
                for manga_link in manga_links:
                    try:
                        manga_url = manga_link.get_attribute('href')
                        title = manga_link.text.strip()
                        
                        if title and manga_url and manga_url not in manga_urls_set:
                            author = "æœªçŸ¥"
                            try:
                                parent = manga_link.find_element(By.XPATH, "./ancestor::*[position()<=5]")
                                author_elem = parent.find_elements(By.XPATH, ".//*[contains(@href, 'users-users_fav-c-')]")
                                if author_elem:
                                    author = author_elem[0].text.strip() or "æœªçŸ¥"
                            except:
                                pass
                            
                            page_count = None
                            try:
                                # æŸ¥æ‰¾åŒ…å«æ¼«ç”»é“¾æ¥çš„çˆ¶å®¹å™¨ï¼Œç„¶åæŸ¥æ‰¾ p.l_detla å…ƒç´ 
                                parent_container = manga_link.find_element(By.XPATH, "./ancestor::*[contains(@class, 'u_listcon') or contains(@class, 'box_cel')]")
                                page_elem = parent_container.find_element(By.CSS_SELECTOR, "p.l_detla")
                                if page_elem:
                                    page_text = page_elem.text
                                    # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                                    page_match = re.search(r'(\d+)\s*P?', page_text)
                                    if page_match:
                                        page_count = int(page_match.group(1))
                            except:
                                pass
                            
                            manga_urls_set.add(manga_url)
                            total_count += 1
                            
                            yield {
                                'title': title,
                                'author': author,
                                'manga_url': manga_url,
                                'page_count': page_count
                            }
                    except Exception as e:
                        print(f"å¤„ç†æ¼«ç”»å¤±è´¥: {e}")
                        continue
            
            print(f"\nâœ“ æ”¶è—å¤¹çˆ¬å–å®Œæˆï¼Œæ€»å…± {total_count} ä¸ªæ¼«ç”»")
            
        except Exception as e:
            print(f"è·å–æ”¶è—å¤¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return


    def get_manga_details(self, manga_url: str) -> Optional[Dict]:
        """è·å–æ¼«ç”»è¯¦æƒ…ï¼ˆé¡µæ•°ã€æ›´æ–°æ—¥æœŸã€å°é¢ç­‰ï¼‰"""
        if not self.driver:
            return None
        
        try:
            self.driver.get(manga_url)
            time.sleep(3)
            
            # è·å–æ ‡é¢˜
            title = None
            try:
                title_elem = self.driver.find_element(By.CSS_SELECTOR, "h2")
                title = title_elem.text.strip()
            except:
                pass
            
            # è·å–é¡µæ•°ï¼ˆä½¿ç”¨classåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
            page_count = None
            try:
                # ä½¿ç”¨ p.l_detla class æŸ¥æ‰¾é¡µæ•°ä¿¡æ¯
                page_elem = self.driver.find_element(By.CSS_SELECTOR, "p.l_detla")
                page_text = page_elem.text  # ä¾‹å¦‚: "é æ•¸ï¼š20P"
                # ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆæ ¼å¼ï¼šé æ•¸ï¼š20 æˆ– é æ•¸ï¼š20Pï¼‰
                page_match = re.search(r'(\d+)\s*P?', page_text)
                if page_match:
                    page_count = int(page_match.group(1))
            except Exception as e:
                print(f"    è·å–é¡µæ•°å¤±è´¥: {e}")
            
            # è·å–ä¸Šä¼ æ—¥æœŸï¼ˆä½¿ç”¨classåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
            updated_at = None
            try:
                # æŸ¥æ‰¾å›¾ç‰‡åˆ—è¡¨é¡¹ï¼ˆä½¿ç”¨ .gallary_item classï¼‰
                gallery_items = self.driver.find_elements(By.CSS_SELECTOR, ".gallary_item")
                if gallery_items:
                    # ä»ç¬¬ä¸€ä¸ªå›¾ç‰‡é¡¹ä¸­æå–æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
                    first_item_text = gallery_items[0].text
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', first_item_text)
                    if date_match:
                        date_str = date_match.group(1)
                        updated_at = datetime.strptime(date_str, '%Y-%m-%d')
            except Exception as e:
                print(f"    è·å–ä¸Šä¼ æ—¥æœŸå¤±è´¥: {e}")
            
            # è·å–å°é¢å›¾ç‰‡URL - å–ç¬¬ä¸€å¼ å›¾ç‰‡çš„ç¼©ç•¥å›¾
            cover_url = None
            try:
                # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡ï¼Œé€šå¸¸ç¬¬ä¸€å¼ æ˜¯å°é¢
                images = self.driver.find_elements(By.CSS_SELECTOR, "img[src*='wnimg']")
                if images:
                    cover_url = images[0].get_attribute('src')
            except Exception as e:
                print(f"    è·å–å°é¢å¤±è´¥: {e}")
            
            return {
                'title': title,
                'manga_url': manga_url,
                'page_count': page_count,
                'updated_at': updated_at,
                'cover_image_url': cover_url
            }
        except Exception as e:
            print(f"è·å–æ¼«ç”»è¯¦æƒ…å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def get_manga_images(self, manga_url: str) -> List[Dict]:
        """è·å–æ¼«ç”»çš„æ‰€æœ‰å›¾ç‰‡URLï¼ŒæŒ‰æ˜¾ç¤ºé¡ºåº
        
        æµç¨‹ï¼š
        1. éå†æ¼«ç”»è¯¦æƒ…é¡µçš„æ‰€æœ‰åˆ†é¡µï¼Œæ”¶é›†æ‰€æœ‰å›¾ç‰‡æŸ¥çœ‹é“¾æ¥ (photos-view-id-xxxxx.html)
        2. é€ä¸ªè®¿é—®è¿™äº›é“¾æ¥ï¼Œä»æ¯ä¸ªé¡µé¢æå–åŸå›¾ URL
        3. ä¸ä½¿ç”¨"ä¸‹æ‹‰é˜…è¯»"ï¼Œå› ä¸ºå®ƒæ˜¯æ‡’åŠ è½½ï¼Œå¤§æ¼«ç”»ä¼šå¯¼è‡´éƒ¨åˆ†å›¾ç‰‡æœªåŠ è½½
        """
        if not self.driver:
            return []
        
        try:
            print(f"\nå¼€å§‹è·å–æ¼«ç”»å›¾ç‰‡: {manga_url}")
            
            # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å›¾ç‰‡æŸ¥çœ‹é“¾æ¥
            # é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§é¡µé¢æ˜¾ç¤ºé¡ºåºæ”¶é›†ï¼Œä¸èƒ½å¯¹é“¾æ¥è¿›è¡Œä»»ä½•æ’åºï¼
            view_urls = []  # ä¿æŒé¡µé¢é¡ºåºçš„é“¾æ¥åˆ—è¡¨
            view_urls_set = set()  # ç”¨äºå¿«é€Ÿå»é‡
            page_num = 1
            
            # ä»ç¬¬ä¸€é¡µå¼€å§‹
            current_url = manga_url
            visited_page_urls = set()
            
            while True:
                # é¿å…é‡å¤è®¿é—®åŒä¸€åˆ†é¡µ
                if current_url in visited_page_urls:
                    print(f"  æ£€æµ‹åˆ°é‡å¤URLï¼Œåœæ­¢æ‰«æ")
                    break
                
                print(f"  æ‰«æç¬¬ {page_num} é¡µ: {current_url}")
                self.driver.get(current_url)
                visited_page_urls.add(current_url)
                time.sleep(2)
                
                # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æŸ¥çœ‹é“¾æ¥ (photos-view-id-xxxxx.html)
                # æ³¨æ„ï¼šfind_elements è¿”å›çš„é¡ºåºå°±æ˜¯é¡µé¢ä¸Šçš„æ˜¾ç¤ºé¡ºåº
                view_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-view-id-']")
                
                if not view_links:
                    if page_num == 1:
                        print(f"    âœ— ç¬¬ 1 é¡µæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡é“¾æ¥")
                    else:
                        print(f"    ç¬¬ {page_num} é¡µæ²¡æœ‰æ›´å¤šå›¾ç‰‡ï¼Œåœæ­¢æ‰«æ")
                    break
                
                # æŒ‰ç…§é¡µé¢é¡ºåºæå–é“¾æ¥ï¼Œä½¿ç”¨ set å¿«é€Ÿå»é‡
                page_view_count = 0
                for link in view_links:
                    url = link.get_attribute('href')
                    # å»é‡ï¼šåªæ·»åŠ æœªè§è¿‡çš„é“¾æ¥ï¼Œä½†ä¿æŒé¡ºåº
                    if url and 'photos-view-id-' in url and url not in view_urls_set:
                        view_urls.append(url)
                        view_urls_set.add(url)
                        page_view_count += 1
                
                print(f"    æ‰¾åˆ° {page_view_count} ä¸ªå›¾ç‰‡é“¾æ¥ï¼ˆæ€»è®¡: {len(view_urls)}ï¼‰")
                
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆä½¿ç”¨HTMLå…ƒç´ å’ŒClassåç§°ï¼Œé¿å…æ±‰å­—å­—ç¬¦ä¸²ï¼‰
                next_page_url = None
                try:
                    # é€šè¿‡åˆ†é¡µå™¨ç»“æ„æŸ¥æ‰¾ï¼ˆä½¿ç”¨classåç§°ï¼‰
                    paginator = self.driver.find_element(By.CSS_SELECTOR, ".paginator")
                    if paginator:
                        # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥ï¼ˆåœ¨paginatorå†…çš„aæ ‡ç­¾ï¼ŒåŒ…å«photos-indexå’Œ-page-ï¼‰
                        page_links = paginator.find_elements(By.CSS_SELECTOR, "a[href*='photos-index'][href*='-page-']")
                        for link in page_links:
                            href = link.get_attribute('href')
                            if href and href not in visited_page_urls:
                                next_page_url = href
                                break
                except:
                    pass
                
                if not next_page_url:
                    try:
                        # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„åˆ†é¡µé“¾æ¥
                        all_page_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='photos-index'][href*='-page-']")
                        for link in all_page_links:
                            href = link.get_attribute('href')
                            if href and href not in visited_page_urls:
                                next_page_url = href
                                break
                    except:
                        pass
                
                if not next_page_url:
                    print(f"    æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œæ‰«æå®Œæˆ")
                    break
                
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„ï¼ˆå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                if next_page_url.startswith('/'):
                    base = self.base_url.rstrip('/')
                    next_page_url = f"{base}{next_page_url}"
                elif not next_page_url.startswith('http'):
                    base = self.base_url.rstrip('/')
                    next_page_url = f"{base}/{next_page_url}"
                
                current_url = next_page_url
                if not current_url:
                    print(f"    ä¸‹ä¸€é¡µé“¾æ¥æ— æ•ˆï¼Œåœæ­¢æ‰«æ")
                    break
                
                page_num += 1
                
                # å®‰å…¨é™åˆ¶ï¼šæœ€å¤š 100 é¡µ
                if page_num > 100:
                    print(f"    è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ (100 é¡µ)")
                    break
            
            print(f"\nå…±æ”¶é›†åˆ° {len(view_urls)} ä¸ªå›¾ç‰‡é“¾æ¥")
            
            if not view_urls:
                print("âœ— æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡é“¾æ¥")
                return []
            
            # ç¬¬äºŒæ­¥ï¼šé€ä¸ªè®¿é—®é“¾æ¥è·å–åŸå›¾
            images = []
            
            for idx, view_url in enumerate(view_urls, 1):
                try:
                    print(f"  [{idx}/{len(view_urls)}] è·å–åŸå›¾...")
                    self.driver.get(view_url)
                    time.sleep(1.5)
                    
                    # æŸ¥æ‰¾åŸå›¾
                    # åŸå›¾ç‰¹å¾ï¼šsrc åŒ…å« wnimgï¼Œä¸”è·¯å¾„ä¸º /data/.../xxx.jpg (ä¸å« /t/)
                    img_elems = self.driver.find_elements(By.CSS_SELECTOR, "img[src*='wnimg']")
                    original_url = None
                    
                    for img_elem in img_elems:
                        src = img_elem.get_attribute('src')
                        # è¿‡æ»¤æ‰ç¼©ç•¥å›¾ (åŒ…å« /t/) å’Œå…¶ä»–éåŸå›¾
                        if src and '/data/' in src and '/t/' not in src:
                            original_url = src
                            break
                    
                    if original_url:
                        # è·å–æ–‡ä»¶æ‰©å±•å
                        ext = original_url.split('.')[-1].split('?')[0] if '.' in original_url else 'jpg'
                        images.append({
                            'index': idx,
                            'url': original_url,
                            'filename': f"{idx:04d}.{ext}"
                        })
                        print(f"    âœ“ {original_url[:70]}...")
                    else:
                        print(f"    âœ— æœªæ‰¾åˆ°åŸå›¾")
                        
                except Exception as e:
                    print(f"    âœ— è·å–å¤±è´¥: {e}")
                    continue
            
            print(f"\nâœ“ æˆåŠŸè·å– {len(images)}/{len(view_urls)} å¼ åŸå›¾")
            return images
            
        except Exception as e:
            print(f"è·å–æ¼«ç”»å›¾ç‰‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            self.driver = None
